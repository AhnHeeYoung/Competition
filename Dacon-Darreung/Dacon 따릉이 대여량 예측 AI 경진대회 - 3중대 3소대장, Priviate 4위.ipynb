{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4546593",
   "metadata": {},
   "source": [
    "# Dacon 따릉이 대여량 예측 AI 경진대회 - 3중대 3소대장, Priviate 4위\n",
    "\n",
    "<br />\n",
    "\n",
    "\n",
    "## ※※ 전체 Training, Test Code는 제 [Github](https://github.com/AhnHeeYoung/Competition/tree/master/Dacon-Darreung)에 정리해 놓았습니다. ※※\n",
    "\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "\n",
    "### 0. 요약\n",
    "- 총 4개 모델(신경망, lightgbm, xgboost, catboost)에 대한 앙상블.     \n",
    "- 각 모델을 훈련하는 과정에서 자체적인 실험에 따라 성능이 잘나오는 Feature를 생성(코드 참고).   \n",
    "- 신경망 같은 경우 자체적으로 구성한 모델을 사용하였으며 10개의 다른 seed에서 훈련한 10개의 신경망 중 best 3개를 앙상블함.   \n",
    "  RAdam optimizer, cosine scheduler, nmae loss를 사용.   \n",
    "- lightgbm, xgboost, catboost 같은 경우 optuna 사용.   \n",
    "- 많은 성능향상을 가져온 부분은   \n",
    "  1. 신경망 training 시 nmae loss 사용\n",
    "  2. 자체적으로 만든 feature들\n",
    "  3. 필요없는 변수 제거\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "\n",
    "### 1. 데이터 전처리, Feature Engineering 및 모델 훈련\n",
    "\n",
    "### 1-1. 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47863d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from xgboost import XGBRegressor, plot_tree, plot_importance\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostClassifier, Pool, CatBoostRegressor\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from itertools import permutations, combinations\n",
    "\n",
    "def NMAE(true, pred):\n",
    "    score = np.mean(np.abs(true-pred) / true)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_nmae(pred, y):\n",
    "    nmae = np.mean(abs(pred-y)/y)\n",
    "    return nmae\n",
    "\n",
    "\n",
    "def nmae_loss(pred, true):\n",
    "    nmae = ((pred - true).abs() / true).mean()\n",
    "    return nmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b934c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "  # 총 데이터의 개수를 리턴\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.x)\n",
    "\n",
    "  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x = torch.from_numpy(self.x[idx]).type(torch.FloatTensor)\n",
    "        y = torch.FloatTensor([self.y[idx] / 1000])\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class NN(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(NN,self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.layer1 = nn.Linear(127, self.dim, bias=True)\n",
    "        self.layer2 = nn.Linear(self.dim, int(self.dim / 2), bias=True)\n",
    "        self.layer3 = nn.Linear(int(self.dim / 2), int(self.dim / 4), bias=True)\n",
    "        self.layer4 = nn.Linear(int(self.dim / 4), int(self.dim / 8), bias=True)\n",
    "        self.layer5 = nn.Linear(int(self.dim / 8), 1, bias=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer5(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "trainset_original = 'data/train.csv'\n",
    "trainset = trainset_original.iloc[:1050, :]\n",
    "testset = trainset_original.iloc[1050:, :]\n",
    "\n",
    "\n",
    "trainset['precipitation'] = trainset['precipitation'].fillna(trainset['precipitation'].median())\n",
    "trainset[\"PM10\"] = trainset[\"PM10\"].fillna(trainset[\"PM10\"].median())\n",
    "trainset[\"PM2.5\"] = trainset[\"PM2.5\"].fillna(trainset[\"PM2.5\"].median())\n",
    "trainset[\"sunshine_sum\"] = trainset[\"sunshine_sum\"].fillna(trainset[\"sunshine_sum\"].median())\n",
    "\n",
    "testset['precipitation'] = testset['precipitation'].fillna(testset['precipitation'].median())\n",
    "testset['sunshine_sum'] = testset['sunshine_sum'].fillna(testset['sunshine_sum'].median())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainset['date'] = pd.to_datetime(trainset['date'])\n",
    "trainset['Year'] = trainset['date'].dt.year\n",
    "trainset['Month'] = trainset['date'].dt.month\n",
    "trainset['day'] = trainset['date'].dt.day\n",
    "\n",
    "\n",
    "testset['date'] = pd.to_datetime(testset['date'])\n",
    "testset[\"Year\"] = testset['date'].dt.year\n",
    "testset['Month'] = testset['date'].dt.month\n",
    "testset[\"day\"] = testset['date'].dt.day\n",
    "\n",
    "\n",
    "trainset.drop(['date'] , axis = 1 , inplace = True)\n",
    "testset.drop(['date'] , axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f972ddbc",
   "metadata": {},
   "source": [
    "### 신경망 Feature 생성. 자체적인 실험에 의해 최적의 성능을 내는 변수 조합 선택."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917568f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns1 = ['precipitation', 'temp_mean', 'temp_highest', 'temp_lowest', 'sunshine_sum', 'sunshine_rate', 'PM10', 'PM2.5', 'humidity', 'Year', 'Month']\n",
    "target_columns2 = ['precipitation', 'temp_mean', 'temp_highest', 'temp_lowest', 'sunshine_sum', 'sunshine_rate', 'PM10', 'PM2.5', 'humidity', 'Year', 'Month']\n",
    "target_columns3 = ['PM10', 'PM2.5', 'Year', 'Month']\n",
    "\n",
    "\n",
    "combis1 = list(combinations(target_columns1, 2))\n",
    "combis2 = list(combinations(target_columns2, 2))\n",
    "combis3 = list(combinations(target_columns3, 2))\n",
    "\n",
    "for com in combis1:\n",
    "    trainset['{}*{}'.format(com[0], com[1])] = trainset['{}'.format(com[0])] * trainset['{}'.format(com[1])]\n",
    "    testset['{}*{}'.format(com[0], com[1])] = testset['{}'.format(com[0])] * testset['{}'.format(com[1])]\n",
    "\n",
    "for com in combis2:\n",
    "    trainset['{}+{}'.format(com[0], com[1])] = trainset['{}'.format(com[0])] + trainset['{}'.format(com[1])]\n",
    "    testset['{}+{}'.format(com[0], com[1])] = testset['{}'.format(com[0])] + testset['{}'.format(com[1])]\n",
    "\n",
    "for com in combis3:\n",
    "    trainset['{}/{}'.format(com[0], com[1])] = trainset['{}'.format(com[0])] / trainset['{}'.format(com[1])]\n",
    "    testset['{}/{}'.format(com[0], com[1])] = testset['{}'.format(com[0])] / testset['{}'.format(com[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208ca3a0",
   "metadata": {},
   "source": [
    "### 필요없는 변수 제거, y 변수 분리 및 데이터 표준화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a2ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.drop(['wind_mean', 'wind_max', 'day'] , axis = 1 , inplace = True)\n",
    "testset.drop(['wind_mean', 'wind_max', 'day'] , axis = 1 , inplace = True)\n",
    "\n",
    "Y = trainset['rental'].values\n",
    "trainset.drop(['rental'] , axis = 1 , inplace = True)\n",
    "\n",
    "test_y_real = testset['rental'].values\n",
    "testset.drop(['rental'] , axis = 1 , inplace = True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "trainset_nn = scaler.fit_transform(trainset)\n",
    "testset_nn = scaler.transform(testset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba16d68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2544d7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7f055ab",
   "metadata": {},
   "source": [
    "### 신경망 Traning 코드.    \n",
    "### amp를 이용한 훈련, RAdam optimizer, cosine scheduler, nmae loss 등 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b377dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.cuda.amp as amp\n",
    "scaler = amp.GradScaler()\n",
    "\n",
    "nn_seed = np.arange(10) + 1\n",
    "seed_best_nmae = []\n",
    "best = []\n",
    "for s in nn_seed:\n",
    "\n",
    "    print(\"Seed : {}\".format(s))\n",
    "    print('\\n')\n",
    "\n",
    "    now = time.localtime()\n",
    "    save = '{}_{}_{}_{}_{}_{}'.format(now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "    os.makedirs('checkpoint/{}'.format(save), exist_ok=True)\n",
    "\n",
    "    device = 'cuda'\n",
    "    model = NN(1024).to(device)\n",
    "\n",
    "    train_dataset = CustomDataset(trainset_nn, Y)\n",
    "    test_dataset = CustomDataset(testset_nn, test_y_real)\n",
    "\n",
    "    batch_size = 8\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    num_epochs = 100\n",
    "    # criterion = nn.MSELoss()\n",
    "    criterion = nn.L1Loss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#     optimizer = torch.optim.RAdam(model.parameters(), lr=0.001)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    total_steps = int(len(train_dataset)*num_epochs/batch_size)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, total_steps, eta_min=1e-7)\n",
    "\n",
    "\n",
    "\n",
    "    # train model\n",
    "\n",
    "    nmae_all = []\n",
    "    pred_all = []\n",
    "\n",
    "    best_nmae = 1\n",
    "    for epoch in range(0, num_epochs):\n",
    "\n",
    "        #print(\"Current LR : {:.6f}\".format(scheduler.get_lr()[0]))\n",
    "\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            x = batch[0].to(device)\n",
    "            y = batch[1].to(device)\n",
    "\n",
    "            with amp.autocast():\n",
    "                output = model(x)\n",
    "\n",
    "            loss = nmae_loss(output, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)    \n",
    "            scaler.update() \n",
    "\n",
    "        model.eval()\n",
    "        output_all = []\n",
    "        for i, batch in enumerate(test_dataloader):\n",
    "            x = batch[0].to(device)\n",
    "            y = batch[1].to(device)\n",
    "\n",
    "            output = model(x)\n",
    "            output_all += output.cpu().detach()[:, 0].tolist()\n",
    "\n",
    "        pred = np.array(output_all) * 1000\n",
    "\n",
    "        pred_all += [pred]\n",
    "        nmae_all += [get_nmae(pred, test_y_real)]\n",
    "\n",
    "\n",
    "        nmae = get_nmae(pred, test_y_real)\n",
    "        if nmae < best_nmae:\n",
    "            best_nmae = nmae\n",
    "            torch.save(model.state_dict(), 'checkpoint/{}/epoch_{}_mae_{:.6f}.pth'.format(save, epoch + 1, get_nmae(pred, test_y_real)))\n",
    "#             print(\"epoch : {}, test nmae : {:.6f}\".format(epoch + 1, get_nmae(pred, test_y_real)))\n",
    "#             print('\\n')\n",
    "\n",
    "    min_idx = np.argmin(np.array(nmae_all))\n",
    "    print(\"Best epoch : {}, Best nmae : {:.6f}\".format(min_idx + 1, np.array(nmae_all)[min_idx]))\n",
    "    print('\\n')\n",
    "\n",
    "    seed_best_nmae += [np.array(nmae_all)[min_idx]]\n",
    "\n",
    "    best += [np.array(nmae_all)[min_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2a17d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d001ce5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6df869b9",
   "metadata": {},
   "source": [
    "### 1-2. Lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32878797",
   "metadata": {},
   "source": [
    "### 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b806334",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_original = 'data.csv'\n",
    "trainset = trainset_original.iloc[:1050, :]\n",
    "testset = trainset_original.iloc[1050:, :]\n",
    "\n",
    "\n",
    "trainset['precipitation'] = trainset['precipitation'].fillna(trainset['precipitation'].median())\n",
    "trainset[\"PM10\"] = trainset[\"PM10\"].fillna(trainset[\"PM10\"].median())\n",
    "trainset[\"PM2.5\"] = trainset[\"PM2.5\"].fillna(trainset[\"PM2.5\"].median())\n",
    "trainset[\"sunshine_sum\"] = trainset[\"sunshine_sum\"].fillna(trainset[\"sunshine_sum\"].median())\n",
    "\n",
    "testset['precipitation'] = testset['precipitation'].fillna(testset['precipitation'].median())\n",
    "testset['sunshine_sum'] = testset['sunshine_sum'].fillna(testset['sunshine_sum'].median())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainset['date'] = pd.to_datetime(trainset['date'])\n",
    "trainset['Year'] = trainset['date'].dt.year\n",
    "trainset['Month'] = trainset['date'].dt.month\n",
    "trainset['day'] = trainset['date'].dt.day\n",
    "\n",
    "\n",
    "testset['date'] = pd.to_datetime(testset['date'])\n",
    "testset[\"Year\"] = testset['date'].dt.year\n",
    "testset['Month'] = testset['date'].dt.month\n",
    "testset[\"day\"] = testset['date'].dt.day\n",
    "\n",
    "\n",
    "trainset.drop(['date'] , axis = 1 , inplace = True)\n",
    "testset.drop(['date'] , axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9810fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916c7e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa90cce5",
   "metadata": {},
   "source": [
    "### Lightgbm Feature 생성. 자체적인 실험에 의해 최적의 성능을 내는 변수 조합 선택."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb10e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns1 = ['precipitation', 'temp_mean', 'temp_highest', 'temp_lowest', 'sunshine_sum', 'sunshine_rate', 'PM10', 'PM2.5', 'humidity', 'Year', 'Month']\n",
    "target_columns2 = ['precipitation', 'temp_mean', 'temp_highest', 'temp_lowest', 'sunshine_sum', 'sunshine_rate', 'PM10', 'PM2.5', 'humidity', 'Year', 'Month']\n",
    "target_columns3 = ['PM10', 'PM2.5', 'Year', 'Month']\n",
    "\n",
    "\n",
    "combis1 = list(combinations(target_columns1, 2))\n",
    "combis2 = list(combinations(target_columns2, 2))\n",
    "combis3 = list(combinations(target_columns3, 2))\n",
    "\n",
    "for com in combis1:\n",
    "    trainset['{}*{}'.format(com[0], com[1])] = trainset['{}'.format(com[0])] * trainset['{}'.format(com[1])]\n",
    "    testset['{}*{}'.format(com[0], com[1])] = testset['{}'.format(com[0])] * testset['{}'.format(com[1])]\n",
    "\n",
    "for com in combis2:\n",
    "    trainset['{}+{}'.format(com[0], com[1])] = trainset['{}'.format(com[0])] + trainset['{}'.format(com[1])]\n",
    "    testset['{}+{}'.format(com[0], com[1])] = testset['{}'.format(com[0])] + testset['{}'.format(com[1])]\n",
    "\n",
    "for com in combis3:\n",
    "    trainset['{}/{}'.format(com[0], com[1])] = trainset['{}'.format(com[0])] / trainset['{}'.format(com[1])]\n",
    "    testset['{}/{}'.format(com[0], com[1])] = testset['{}'.format(com[0])] / testset['{}'.format(com[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4472fa33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7b1305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5d4ce98",
   "metadata": {},
   "source": [
    "### 필요없는 변수 제거 및 y 변수 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dc3ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.drop(['day' ,'wind_max', 'wind_mean'] , axis = 1 , inplace = True )\n",
    "testset.drop(['day', 'wind_max', 'wind_mean'] , axis = 1 , inplace = True )\n",
    "\n",
    "# trainset.drop(['day' ,'wind_max'] , axis = 1 , inplace = True )\n",
    "# testset.drop(['day', 'wind_max'] , axis = 1 , inplace = True )\n",
    "\n",
    "Y = trainset['rental'].values\n",
    "trainset.drop(['rental'] , axis = 1 , inplace = True)\n",
    "\n",
    "test_y_real = testset['rental'].values\n",
    "testset.drop(['rental'] , axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e816e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde1d3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d5d5edd",
   "metadata": {},
   "source": [
    "### LGBM 훈련 코드. optuna 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c54af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    param = {\n",
    "        \"boosting_type\": trial.suggest_categorical('boosting_type', ['gbdt', 'dart']),\n",
    "        \"random_state\": 357,\n",
    "        \"verbosity\": -1,\n",
    "        \"learning_rate\": trial.suggest_uniform('learning_rate', 0.1, 0.5),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 300, step=10),\n",
    "        \"objective\": \"mae\",\n",
    "        \"metric\": \"nmae\",\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.1, 1),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.01, 1),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 8),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 50, 150),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "        \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 1, 10),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 800, 1000),\n",
    "    }     \n",
    "\n",
    "\n",
    "    lgbm = LGBMRegressor(**param)\n",
    "    lgbm.fit(trainset, Y, verbose=False)\n",
    "\n",
    "    ex = ((np.arange(10) + 1) / 10) + 1\n",
    "    result_all = []\n",
    "    for e in ex:\n",
    "        test_pred = lgbm.predict(testset)\n",
    "        test_pred *= e\n",
    "\n",
    "        test_nmae = get_nmae(test_pred, test_y_real)\n",
    "        result_all += [test_nmae]\n",
    "\n",
    "    min_idx_lgbm = np.argmin(np.array(result_all))\n",
    "    best_e = ex[min_idx_lgbm]\n",
    "\n",
    "    best_score = np.array(result_all)[min_idx_lgbm]\n",
    "\n",
    "    return best_score\n",
    "\n",
    "\n",
    "sampler = TPESampler(seed=357)\n",
    "study = optuna.create_study(\n",
    "    study_name = 'lgbm',\n",
    "    direction = 'minimize',\n",
    "    sampler = sampler,\n",
    ")\n",
    "study.optimize(objective, n_trials=30000)\n",
    "print(\"Best Score:\", study.best_value)\n",
    "print(\"Best Params\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aa4e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79105cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50252a8c",
   "metadata": {},
   "source": [
    "### 1-3. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195155b9",
   "metadata": {},
   "source": [
    "### 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0f667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_original = 'data.csv'\n",
    "trainset = trainset_original.iloc[:1050, :]\n",
    "testset = trainset_original.iloc[1050:, :]\n",
    "\n",
    "\n",
    "trainset['precipitation'] = trainset['precipitation'].fillna(trainset['precipitation'].median())\n",
    "trainset[\"PM10\"] = trainset[\"PM10\"].fillna(trainset[\"PM10\"].median())\n",
    "trainset[\"PM2.5\"] = trainset[\"PM2.5\"].fillna(trainset[\"PM2.5\"].median())\n",
    "trainset[\"sunshine_sum\"] = trainset[\"sunshine_sum\"].fillna(trainset[\"sunshine_sum\"].median())\n",
    "\n",
    "testset['precipitation'] = testset['precipitation'].fillna(testset['precipitation'].median())\n",
    "testset['sunshine_sum'] = testset['sunshine_sum'].fillna(testset['sunshine_sum'].median())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainset['date'] = pd.to_datetime(trainset['date'])\n",
    "trainset['Year'] = trainset['date'].dt.year\n",
    "trainset['Month'] = trainset['date'].dt.month\n",
    "trainset['day'] = trainset['date'].dt.day\n",
    "\n",
    "\n",
    "testset['date'] = pd.to_datetime(testset['date'])\n",
    "testset[\"Year\"] = testset['date'].dt.year\n",
    "testset['Month'] = testset['date'].dt.month\n",
    "testset[\"day\"] = testset['date'].dt.day\n",
    "\n",
    "\n",
    "trainset.drop(['date'] , axis = 1 , inplace = True)\n",
    "testset.drop(['date'] , axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303bbf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e4c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab65e2bb",
   "metadata": {},
   "source": [
    "### XGBoost Feature 생성. 자체적인 실험에 의해 최적의 성능을 내는 변수 조합 선택."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7092a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "######################        Feature engineering         ######################\n",
    "################################################################################\n",
    "# target_columns1 = ['precipitation', 'temp_mean', 'temp_highest', 'temp_lowest', 'sunshine_sum', 'sunshine_rate', 'PM10', 'PM2.5', 'humidity', 'Year', 'Month', 'day']\n",
    "# target_columns2 = ['precipitation', 'temp_mean', 'temp_highest', 'temp_lowest', 'sunshine_sum', 'sunshine_rate', 'PM10', 'PM2.5', 'humidity', 'Year', 'Month', 'day']\n",
    "# 'precipitation', 'temp_mean', 'temp_highest', 'sunshine_sum', 'sunshine_rate', 'PM10', 'PM2.5', 'humidity', 'Year'  # Combination when multiplying21.22\n",
    "target_columns1 = ['temp_mean', 'temp_lowest', 'sunshine_sum'] # Best Combination when multiplying #0.1793\n",
    "target_columns2 = ['precipitation', 'temp_mean', 'temp_lowest', 'sunshine_sum', 'PM10', 'PM2.5'] # Best Combination when Adding (For LGBM !!!!) # 20.06\n",
    "# target_columns2 = ['precipitation', 'temp_mean', 'temp_lowest', 'sunshine_sum', 'PM10', 'PM2.5', 'Year', 'Month'] # Best Combination when Adding (For LGBM !!!!) # 20.06\n",
    "\n",
    "\n",
    "combis1 = list(combinations(target_columns1, 2))\n",
    "combis2 = list(combinations(target_columns2, 2))\n",
    "\n",
    "# for com in combis1:\n",
    "#     trainset['{}*{}'.format(com[0], com[1])] = trainset['{}'.format(com[0])] * trainset['{}'.format(com[1])]\n",
    "#     testset['{}*{}'.format(com[0], com[1])] = testset['{}'.format(com[0])] * testset['{}'.format(com[1])]\n",
    "\n",
    "for com in combis2:\n",
    "    trainset['{}+{}'.format(com[0], com[1])] = trainset['{}'.format(com[0])] + trainset['{}'.format(com[1])]\n",
    "    testset['{}+{}'.format(com[0], com[1])] = testset['{}'.format(com[0])] + testset['{}'.format(com[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f481668a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1902bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f53354a",
   "metadata": {},
   "source": [
    "### 필요없는 변수 제거 및 y 변수 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5929ff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.drop(['humidity' ,  'day' ,'wind_max', 'wind_mean'] , axis = 1 , inplace = True )\n",
    "testset.drop(['humidity' ,  'day', 'wind_max', 'wind_mean'] , axis = 1 , inplace = True )\n",
    "\n",
    "# trainset.drop(['day' ,'wind_max'] , axis = 1 , inplace = True )\n",
    "# testset.drop(['day', 'wind_max'] , axis = 1 , inplace = True )\n",
    "\n",
    "Y = trainset['rental'].values\n",
    "trainset.drop(['rental'] , axis = 1 , inplace = True)\n",
    "\n",
    "test_y_real = testset['rental'].values\n",
    "testset.drop(['rental'] , axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21bf497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bdcb72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47caccff",
   "metadata": {},
   "source": [
    "### XGBoost 훈련 코드. optuna 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d0f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "\n",
    "    param = {\n",
    "        \"eval_metric\":'mae',\n",
    "        \"booster\":  trial.suggest_categorical('booster',['dart']),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 8),\n",
    "        \"learning_rate\": trial.suggest_uniform('learning_rate', 0.1, 0.3),\n",
    "        'n_estimators': trial.suggest_int(\"n_estimators\", 10, 500),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0., 1.0),\n",
    "        \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.5, 1.0),\n",
    "        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-5, 1),\n",
    "        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-5, 1),\n",
    "        'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.01),     \n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 10, 50),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.1, 1.0, log=True),\n",
    "        \"loss_function\" : \"MAE\",\n",
    "        'random_state' : 357}\n",
    "\n",
    "\n",
    "    xgb = XGBRegressor(**param)\n",
    "    xgb.fit(trainset, Y, verbose=False)\n",
    "\n",
    "    ex = ((np.arange(10) + 1) / 10) + 1\n",
    "    result_all = []\n",
    "    for e in ex:\n",
    "        test_pred = xgb.predict(testset)\n",
    "        test_pred *= e\n",
    "\n",
    "        test_nmae = get_nmae(test_pred, test_y_real)\n",
    "        result_all += [test_nmae]\n",
    "\n",
    "    min_idx_lgbm = np.argmin(np.array(result_all))\n",
    "    best_e = ex[min_idx_lgbm]\n",
    "\n",
    "    best_score = np.array(result_all)[min_idx_lgbm]\n",
    "\n",
    "    return best_score\n",
    "\n",
    "\n",
    "sampler = TPESampler(seed=357)\n",
    "study = optuna.create_study(\n",
    "    study_name = 'xgb',\n",
    "    direction = 'minimize',\n",
    ")\n",
    "study.optimize(objective, n_trials=30000)\n",
    "print(\"Best Score:\", study.best_value)\n",
    "print(\"Best Params\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea2b5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf434cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0760a4c",
   "metadata": {},
   "source": [
    "### 1-4. Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f2e96",
   "metadata": {},
   "source": [
    "### 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eeaaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_original = csv\n",
    "trainset = trainset_original.iloc[:1050, :]\n",
    "testset = trainset_original.iloc[1050:, :]\n",
    "\n",
    "\n",
    "trainset['precipitation'] = trainset['precipitation'].fillna(trainset['precipitation'].median())\n",
    "trainset[\"PM10\"] = trainset[\"PM10\"].fillna(trainset[\"PM10\"].median())\n",
    "trainset[\"PM2.5\"] = trainset[\"PM2.5\"].fillna(trainset[\"PM2.5\"].median())\n",
    "trainset[\"sunshine_sum\"] = trainset[\"sunshine_sum\"].fillna(trainset[\"sunshine_sum\"].median())\n",
    "\n",
    "testset['precipitation'] = testset['precipitation'].fillna(testset['precipitation'].median())\n",
    "testset['sunshine_sum'] = testset['sunshine_sum'].fillna(testset['sunshine_sum'].median())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainset['date'] = pd.to_datetime(trainset['date'])\n",
    "trainset['Year'] = trainset['date'].dt.year\n",
    "trainset['Month'] = trainset['date'].dt.month\n",
    "trainset['day'] = trainset['date'].dt.day\n",
    "\n",
    "\n",
    "testset['date'] = pd.to_datetime(testset['date'])\n",
    "testset[\"Year\"] = testset['date'].dt.year\n",
    "testset['Month'] = testset['date'].dt.month\n",
    "testset[\"day\"] = testset['date'].dt.day\n",
    "\n",
    "\n",
    "trainset.drop(['date'] , axis = 1 , inplace = True)\n",
    "testset.drop(['date'] , axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62900716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ddae58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d27996f",
   "metadata": {},
   "source": [
    "### Catboost Feature 생성. 자체적인 실험에 의해 최적의 성능을 내는 변수 조합 선택."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f51e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "######################        Feature engineering         ######################\n",
    "################################################################################\n",
    "target_columns1 = ['precipitation', 'temp_mean', 'temp_highest', 'temp_lowest', 'sunshine_sum', 'sunshine_rate', 'PM10',\n",
    "                   'PM2.5', 'humidity', 'Year', 'Month']\n",
    "target_columns2 = ['precipitation', 'temp_mean', 'temp_highest', 'temp_lowest', 'sunshine_sum', 'sunshine_rate', 'PM10',\n",
    "                   'PM2.5', 'humidity', 'Year', 'Month']\n",
    "# target_columns3 = ['PM10', 'PM2.5', 'Year', 'Month']\n",
    "target_columns3 = ['PM10', 'PM2.5', 'Year', 'Month']\n",
    "\n",
    "combis1 = list(combinations(target_columns1, 2))\n",
    "combis2 = list(combinations(target_columns2, 2))\n",
    "combis3 = list(combinations(target_columns3, 2))\n",
    "\n",
    "for com in combis1:\n",
    "    trainset['{}*{}'.format(com[0], com[1])] = trainset['{}'.format(com[0])] * trainset['{}'.format(com[1])]\n",
    "    testset['{}*{}'.format(com[0], com[1])] = testset['{}'.format(com[0])] * testset['{}'.format(com[1])]\n",
    "\n",
    "for com in combis2:\n",
    "    trainset['{}+{}'.format(com[0], com[1])] = trainset['{}'.format(com[0])] + trainset['{}'.format(com[1])]\n",
    "    testset['{}+{}'.format(com[0], com[1])] = testset['{}'.format(com[0])] + testset['{}'.format(com[1])]\n",
    "\n",
    "for com in combis3:\n",
    "    trainset['{}/{}'.format(com[0], com[1])] = trainset['{}'.format(com[0])] / trainset['{}'.format(com[1])]\n",
    "    testset['{}/{}'.format(com[0], com[1])] = testset['{}'.format(com[0])] / testset['{}'.format(com[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f06ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134ce599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8f8dd89",
   "metadata": {},
   "source": [
    "### 필요없는 변수 제거 및 y 변수 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b08ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.drop(['humidity' ,  'day' ,'wind_max', 'wind_mean'] , axis = 1 , inplace = True )\n",
    "testset.drop(['humidity' ,  'day', 'wind_max', 'wind_mean'] , axis = 1 , inplace = True )\n",
    "\n",
    "Y = trainset['rental'].values\n",
    "trainset.drop(['rental'] , axis = 1 , inplace = True)\n",
    "\n",
    "\n",
    "test_y_real = testset['rental'].values\n",
    "testset.drop(['rental'] , axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac3e828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31a5453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04ac67a3",
   "metadata": {},
   "source": [
    "### Catboost 훈련. optuna 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a5a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    param = {\n",
    "        \"random_state\": 357,\n",
    "        'iterations' : trial.suggest_int(\"iterations\", 400, 800),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.1, 0.3),\n",
    "        \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", 30, 50),\n",
    "        \"subsample\" : trial.suggest_float(\"subsample\", 0.3, 0.5),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 8),\n",
    "        'random_strength': trial.suggest_int('random_strength', 60, 80),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.8, 1.0),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"loss_function\" : \"MAE\",\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 400, 1000),\n",
    "    }\n",
    "\n",
    "    # cat_features = [0, 1, 2, 5, 6, 7, 8, 15, 18]\n",
    "    cat = CatBoostRegressor(**param)\n",
    "    cat.fit(trainset, Y, verbose=False)\n",
    "\n",
    "    ex = ((np.arange(10) + 1) / 10) + 1\n",
    "    result_all = []\n",
    "    for e in ex:\n",
    "        test_pred = cat.predict(testset)\n",
    "        test_pred *= e\n",
    "\n",
    "        test_nmae = get_nmae(test_pred, test_y_real)\n",
    "        result_all += [test_nmae]\n",
    "\n",
    "    min_idx_lgbm = np.argmin(np.array(result_all))\n",
    "    best_e = ex[min_idx_lgbm]\n",
    "\n",
    "    best_score = np.array(result_all)[min_idx_lgbm]\n",
    "\n",
    "    return best_score\n",
    "\n",
    "\n",
    "sampler = TPESampler(seed=357)\n",
    "study = optuna.create_study(\n",
    "    study_name = 'catboost',\n",
    "    direction = 'minimize',\n",
    "    sampler = sampler,\n",
    ")\n",
    "study.optimize(objective, n_trials=30000)\n",
    "print(\"Best Score:\",study.best_value)\n",
    "print(\"Best Params\",study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015effed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab2bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4c94880",
   "metadata": {},
   "source": [
    "## 2. Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece3da73",
   "metadata": {},
   "source": [
    "4개 모델을 각각 다른 가중치로 앙상블 하였으며   \n",
    "자세한 코드는 [Github](https://github.com/AhnHeeYoung/Competition/tree/master/Dacon-Darreung) 의 test.py 코드 참고 바랍니다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
